Component,Alarm Name,Metric Name,Statistic,Threshold,Period (seconds),Evaluation Periods,Datapoints to Alarm,Severity,Reasoning
EKS Pods,Pod Pending Count,pod_number_of_containers_pending,Sum,> 5 pods,300,3,3,P2-High,"Pods stuck in Pending state indicate scheduling failures - insufficient CPU/memory, taints/tolerations mismatches, PVC binding failures, or node capacity exhaustion. Threshold 5 pods filters transient pending during deployments vs. persistent scheduling failures. 3/3 datapoints (15 min) confirms sustained issue requiring capacity or constraint resolution."
EKS Pods,Pod Restart Count,pod_restart_count,Anomaly Detection,GreaterThanUpperThreshold,300,3,2,P2-High,"High pod restart rates indicate application crashes, OOMKills, liveness probe failures, or node instability. Anomaly detection adapts to normal restart patterns (zero for stable apps, occasional for apps with probes). Upper threshold detects abnormal restart rates requiring application or infrastructure investigation. Container Insights metric."
EKS Pods,Pod CPU/Memory Over Limit,pod_cpu_utilization_over_pod_limit,Maximum,> 95%,300,2,2,P3-Medium,Pod resource usage approaching limits indicates under-provisioned requests/limits. CPU throttling or OOMKill risk. 95% threshold provides minimal buffer. 2/2 datapoints confirms sustained high utilization vs. brief spikes. Action: increase resource requests/limits or optimize application resource usage. Prevents performance degradation and pod terminations.
