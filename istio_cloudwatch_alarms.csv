Component,Alarm Name,CloudWatch Metric Name,Namespace,Statistic,Dimensions,Threshold,Period (seconds),Evaluation Periods,Datapoints to Alarm,Treat Missing Data,Severity,CloudWatch Metric Query / Math Expression,Terraform Resource Example,Reasoning
Istio,Service Request Rate Drop,istio_requests_total,ContainerInsights/Prometheus,Sum,"ClusterName, Namespace, Service, destination_service",Anomaly: LessThanLowerThreshold,300,2,2,notBreaching,P1-Critical,"ANOMALY_DETECTION_BAND(m1, 8) where m1 = SUM(istio_requests_total)",aws_cloudwatch_metric_alarm with metric_query + anomaly detection,CloudWatch anomaly detection adapts to traffic patterns. LessThanLowerThreshold detects sudden request drops indicating service outages. Use 8 std dev band for production stability. Sum statistic over 5-min period captures total request volume. Filter by destination_service dimension to isolate specific service issues.
Istio,Service Request Rate Spike,istio_requests_total,ContainerInsights/Prometheus,Sum,"ClusterName, Namespace, Service, destination_service",Anomaly: GreaterThanUpperThreshold,300,2,2,notBreaching,P2-High,"ANOMALY_DETECTION_BAND(m1, 6) where m1 = SUM(istio_requests_total)",aws_cloudwatch_metric_alarm with metric_query + anomaly detection,"GreaterThanUpperThreshold with 6 std dev detects traffic spikes from DDoS, retry storms, or legitimate surges. 2 consecutive 5-min periods confirms sustained spike vs. momentary burst. Triggers capacity review and rate limiting evaluation. Lower std dev (6 vs 8) for more sensitive spike detection."
Istio,Service 5xx Error Rate,istio_requests_total,ContainerInsights/Prometheus,Sum,"ClusterName, Namespace, Service, destination_service, response_code",> 10 errors/min (adjust per traffic),300,2,2,notBreaching,P1-Critical,"SEARCH('{ContainerInsights/Prometheus,ClusterName,Namespace,Service} MetricName=""istio_requests_total"" response_code=""5*""', 'Sum', 300)",aws_cloudwatch_metric_alarm with metric_query,"Direct 5xx error count monitoring. Use CloudWatch Metric Search with wildcard response_code=""5*"" to match all 500-599 codes. Threshold 10 errors/min for typical microservice; adjust based on traffic volume. 2/2 datapoints ensures sustained errors. Critical as indicates backend failures impacting users."
Istio,Service 4xx Error Rate,istio_requests_total,ContainerInsights/Prometheus,Sum,"ClusterName, Namespace, Service, destination_service, response_code",> 50 errors/min,300,3,3,notBreaching,P2-High,"SEARCH('{ContainerInsights/Prometheus,ClusterName,Namespace,Service} MetricName=""istio_requests_total"" response_code=""4*""', 'Sum', 300)",aws_cloudwatch_metric_alarm with metric_query,"4xx client errors (401, 403, 404, 400). Higher threshold (50/min) than 5xx as some 4xx expected. Use response_code=""4*"" wildcard. 3/3 datapoints reduces sensitivity to gradual increases. Sudden spikes may indicate API breaking changes, authentication issues, or security probing attempts."
Istio,Service Request Duration P99,istio_request_duration_milliseconds,ContainerInsights/Prometheus,Average,"ClusterName, Namespace, Service, destination_service",> 2000 ms,300,2,2,notBreaching,P1-Critical,"SELECT AVG(istio_request_duration_milliseconds) FROM SCHEMA(""ContainerInsights/Prometheus"") WHERE quantile=""0.99""",aws_cloudwatch_metric_alarm with metric_query,"Latency monitoring using quantile=""0.99"" dimension from Istio histogram metrics. CloudWatch Container Insights with Prometheus scraping exposes percentile metrics as separate dimensions. Average statistic on P99 metric over 5-min window. Threshold 2000ms for user-facing services."
Istio,Circuit Breaker Triggered,envoy_cluster_circuit_breakers_default_rq_open,ContainerInsights/Prometheus,Maximum,"ClusterName, Namespace, Service, envoy_cluster_name",>= 1,60,2,2,breaching,P1-Critical,MAX(envoy_cluster_circuit_breakers_default_rq_open) FROM ContainerInsights/Prometheus,aws_cloudwatch_metric_alarm with standard metric,Circuit breaker open state. Maximum statistic ensures any Envoy proxy reporting open circuit breaker triggers alarm. >= 1 threshold means any non-zero value = service degraded. 2 consecutive 1-min periods confirms not momentary. Treat missing data as breaching to ensure alarm if metric stops reporting.
Istio,Upstream Connection Failures,envoy_cluster_upstream_cx_connect_fail,ContainerInsights/Prometheus,Sum,"ClusterName, Namespace, Service, envoy_cluster_name",> 10 failures/min,300,3,3,notBreaching,P1-Critical,SUM(envoy_cluster_upstream_cx_connect_fail) FROM ContainerInsights/Prometheus,aws_cloudwatch_metric_alarm with standard metric,"Connection establishment failures indicate network issues, pod unavailability, or DNS failures. Sum over 5 minutes provides failure count. Threshold 10 failures/min for services with 100+ rps. 3 consecutive periods confirms persistent connectivity issues requiring investigation of Service endpoints, network policies."
Istio,Request Timeout Count,envoy_cluster_upstream_rq_timeout,ContainerInsights/Prometheus,Sum,"ClusterName, Namespace, Service, envoy_cluster_name",> 20 timeouts/min,300,2,2,notBreaching,P2-High,SUM(envoy_cluster_upstream_rq_timeout) FROM ContainerInsights/Prometheus,aws_cloudwatch_metric_alarm with standard metric,Request timeout count from Envoy upstream timeout metric. Sum over 5 min shows total timeouts. Threshold 20 timeouts/min depends on service traffic volume and tolerance. 2/2 datapoints confirms sustained timeouts. Investigate VirtualService timeout settings and upstream service latency/resource constraints.
